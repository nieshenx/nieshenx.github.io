<html>

<head>
	<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
	<meta name="description" content="Home page of Shen Nie">
	<link rel="stylesheet" href="./files/jemdoc.css" type="text/css">
	<title>Shen Nie</title>
</head>


<body>

<div id="layout-content" style="margin-top:25px">

<table><tbody><tr>
    <td width="670">
        <div id="toptitle"><h1>Shen Nie (聂燊) &nbsp;</h1></div>
        <h3>Ph.D. Student</h3>
        <p>
          <a href="http://ai.ruc.edu.cn/" target="_blank">Gaoling School of Artificial Intelligence</a> <br>
          <a href="https://www.ruc.edu.cn/" target="_blank">Renmin University of China</a> <br>
          Email:  nieshen@ruc.edu.cn<br>
          Links: <a href="https://scholar.google.com/citations?user=mVaNdrsAAAAJ" target="_blank">[Google Scholar]</a>
                 <a href="https://github.com/nieshenx" target="_blank">[Github]</a> <br>
          <br>
      </p>
    </td>

    <td><img src="./files/photo.jpg" border="0" width="200"></td>
</tr></tbody></table>


<h2>Education</h2>
<p>
    <font color="#FF0000">
            I expect to graduate in June 2027 and am seeking post-doctoral or researcher positions.
            If you are interested in diffusion language models or unified diffusion models, I would welcome the opportunity to discuss.
    </font> <br>
    • 2018–2022: Bachelor’s in Computer Science, Xi’an Jiaotong University<br>
  • 2022–(expected 2027): PhD in Artificial Intelligence, Renmin University of China (Advisor: Prof.
  <a href="https://zhenxuan00.github.io/" target="_blank">Chongxuan Li</a>)
</p>


<h2>Research</h2>
    I focus on <strong>deep generative models</strong>, especially <strong>multimodal diffusion models</strong>. <br>

    One of my favorite papers is <a href="https://arxiv.org/pdf/2010.11929" target="_blank">Vision Transformer</a>. ViT taught me that removing inductive biases
    from data (e.g., translation equivariance in images, and the left-to-right paradigm in text) and employing
    large-scale training is beneficial for deep learning algorithms. This insight also aligns with "The Bitter Lesson".<br>

    Therefore, my research focuses on removing inductive biases and developing scalable generative models.

<h3>Selected Publications</h3>
    <ul>
    <li>
      <paper>Large Language Diffusion Models</paper><br>
      <strong>Shen Nie</strong>, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, Chongxuan Li<br>
      Advances in Neural Information Processing Systems <strong>(NeurIPS)</strong>, 2025.<br>
        <font color="#FF0000">Oral, NeurIPS 2025</font> <br>
        <font color="#FF0000">Long Paper Best Paper Award, DeLTa@ICLR 2025</font><br>
        <em>The first diffusion language model comparable to advanced LLMs (e.g., LLaMA).  Earlier than Inception Lab's Mercury and Google's Gemini Diffusion.</em> <br>
      <a href="https://arxiv.org/abs/2502.09992" target="_blank">[Paper]</a>
    </li>
</ul>

    <ul>
    <li>
      <paper>All are Worth Words: A ViT Backbone for Diffusion Models</paper><br>
      Fan Bao, <strong>Shen Nie</strong>, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, Jun Zhu<br>
      Computer Vision and Pattern Recognition Conference <strong>(CVPR)</strong>, 2023.<br>
        <em>The first diffusion transformer.  Earlier than Openai Sora's DiT.</em> <br>
      <a href="https://arxiv.org/pdf/2209.12152" target="_blank">[Paper]</a>
    </li>
</ul>

<h3>Full Publications</h3>

<ul>
    <li>
      <paper>Scaling up masked diffusion models on text</paper><br>
      <strong>Shen Nie</strong>, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Ming Lin, Chongxuan Li<br>
      International Conference on Learning Representations <strong>(ICLR)</strong>, 2025.<br>
      <a href="https://arxiv.org/abs/2410.18514" target="_blank">[Paper]</a>
    </li>
</ul>
    <ul>
    <li>
      <paper>The blessing of randomness: Sde beats ode in general diffusion-based image editing</paper><br>
      <strong>Shen Nie</strong>, Hanzhong Guo, Cheng Lu, Yuhao Zhou, Chenyu Zheng, Chongxuan Li<br>
      International Conference on Learning Representations <strong>(ICLR)</strong>, 2024.<br>
      <a href="https://arxiv.org/abs/2311.01410" target="_blank">[Paper]</a>
    </li>
</ul>

<ul>
    <li>
      <paper>Your absorbing discrete diffusion secretly models the conditional distributions of clean data</paper><br>
      Jingyang Ou, <strong>Shen Nie</strong>, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhanguo Li, Chongxuan Li<br>
      International Conference on Learning Representations <strong>(ICLR)</strong>, 2025.<br>
      <a href="https://arxiv.org/abs/2406.03736" target="_blank">[Paper]</a>
    </li>
</ul>
    <ul>
    <li>
      <paper>LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language Diffusion Models</paper><br>
      Fengqi Zhu, Rongzhen Wang, <strong>Shen Nie</strong>, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, Chongxuan Li<br>
      arXiv preprint, 2025.<br>
      <a href="https://arxiv.org/abs/2505.19223" target="_blank">[Paper]</a>
    </li>
</ul>
    <ul>
    <li>
      <paper>Llada-v: Large language diffusion models with visual instruction tuning</paper><br>
      Zebin You, <strong>Shen Nie</strong>, Xiaolu Zhang, Jun Hu, Jun Zhou, Zhiwu Lu, Ji-Rong Wen, Chongxuan Li<br>
      arXiv preprint, 2025.<br>
      <a href="https://arxiv.org/abs/2505.16933" target="_blank">[Paper]</a>
    </li>
</ul>
    <ul>
    <li>
      <paper>Ultrallada: Scaling the context length to 128k for diffusion large language models</paper><br>
      Guangxin He, <strong>Shen Nie</strong>, Fengqi Zhu, Yuankang Zhao, Tianyi Bai, Ran Yan, Jie Fu, Chongxuan Li, Binhang Yuan<br>
      arXiv preprint, 2025.<br>
      <a href="https://arxiv.org/abs/2510.10481" target="_blank">[Paper]</a>
    </li>
</ul>
    <ul>
    <li>
      <paper>Masked Diffusion Models as Energy Minimization</paper><br>
      Sitong Chen, <strong>Shen Nie</strong>, Jjiacheng Sun, Zijing Feng, Zhengguo Li, Ji-Rong Wen, Chongxuan Li<br>
      Advances in Neural Information Processing Systems <strong>(NeurIPS)</strong>, 2025.<br>
      <a href="https://arxiv.org/abs/2509.13866" target="_blank">[Paper]</a>
    </li>
</ul>
    <ul>
    <li>
      <paper>Unifying bayesian flow networks and diffusion models through stochastic differential equations</paper><br>
      Kaiwen Xue, Yuhao Zhou, <strong>Shen Nie</strong>, Xu Min, Xiaolu Zhang, Jun Zhou, Chongxuan Li<br>
      International Conference on Machine Learning <strong>(ICML)</strong>, 2024.<br>
      <a href="https://arxiv.org/abs/2404.15766" target="_blank">[Paper]</a>
    </li>
</ul>
    <ul>
    <li>
      <paper>Real-time identity defenses against malicious personalization of diffusion models</paper><br>
      Hanzhong Guo, <strong>Shen Nie</strong>, Chao Du, Tianyu Pang, Hao Sun, Chongxuan Li<br>
      arXiv preprint, 2024.<br>
      <a href="https://arxiv.org/abs/2412.09844" target="_blank">[Paper]</a>
    </li>
</ul>
    <ul>
    <li>
      <paper>One transformer fits all distributions in multi-modal diffusion at scale</paper><br>
      Fan Bao, <strong>Shen Nie</strong>, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, Jun Zhu<br>
      International Conference on Machine Learning <strong>(ICML)</strong>, 2023.<br>
      <a href="https://arxiv.org/abs/2303.06555" target="_blank">[Paper]</a>
    </li>
</ul>


<h2>Experience</h2>
    <ul>
    <li>
      <strong>ByteDance</strong><br>
      Top-seed Research Intern, 2025.03 - Present<br>
      <em>Focus: Diffusion Language Models</em>
    </li>
</ul>
    <ul>
    <li>
      <strong>Ant Group</strong><br>
      Research Intern, 2024.12 - 2025.02<br>
      <em>Focus: Diffusion Language Models</em>
    </li>
</ul>
    <ul>
    <li>
      <strong>Sea AI Lab</strong><br>
      Research Intern, 2024.03 - 2024.11<br>
      <em>Focus: Diffusion Language Models</em>
    </li>
</ul>
    <ul>
    <li>
      <strong>Kuaishou (Kwai)</strong><br>
      Research Intern, 2023.11 - 2024.01<br>
      <em>Focus: Text-to-Image/Video Models</em>
    </li>
</ul>
    <ul>
    <li>
      <strong>Shengshu</strong><br>
      Research Intern, 2023.03 - 2023.10<br>
      <em>Focus: Text-to-Image/Video Models, Unified Diffusion Model</em>
    </li>
</ul>


<h2>Current Interests</h2>
    <ol>
    <li>
      <strong>Infra.</strong> Infra is the key in today's AI. I am currently learning it.
    </li>

    <li>
      <strong>RL for Diffusion Language Models.</strong> Reinforcement Learning for dLLMs shares roots with autoregressive models, but presents many significant and fundamental differences.
    </li>

    <li>
      <strong>Normalizing Flow.</strong> For example, <a href="https://arxiv.org/abs/2510.23588" target="_blank">[FARMER]</a>. This is a very interesting topic and might be a reliable method or component for future unified models.
    </li>
</ol>



<h2>Academic Services</h2>

Conference reviewer for ICLR, ICML, NeurIPS, CVPR, MM, TPAMI
</div>

<div id="footer">
	<div id="footer-text"></div>
</div>
&copy 2025 Shen Nie

<br>
<script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=BzKyRov0uB7DituC_53E1B0Y45qDcoe0C0SaKD-4lBA&cl=ffffff&w=a"></script>

</body>

</html>